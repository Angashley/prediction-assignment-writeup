---
title: "Qualitative Activity Recognition using the Weight Lifting Exercise Dataset"
author: "Ashley You"
date: "16th July 2017"
output: html_document
---

## 1. Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit, it is now possible to collect a large amount of data about personal activity relatively inexpensively. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants to predict the manner in which they did the exercise as labelled by the `classe` variable. Weight Lifting Exercise Dataset^[1]^ is used in the project. 

The main objectives of this project are as follows:

1) Preprocess the weight lifting exercise dataset;
2) Use the `XGBoost` algorithm to build a prediction model;
3) Evaluate the model performance, including accuracy, out of sample error and features importance list;
4) Use the prediction model to predict the test data. 

## 2. Data Preprocessing

### 2.1 Downloading data sets and reading the files

```{r}
trainingURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"

testingURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

if (!file.exists("pml_training.csv")){
        download.file(trainingURL, 
                      destfile = "pml_training.csv")}

if (!file.exists("pml_testing.csv")){
        download.file(testingURL, 
                      destfile = "pml_testing.csv")}

# Missing values "#DIV/0!", "", or "NA" are all coded as NA for consistency
pml_building <- read.csv("pml_training.csv", na.strings = c("","NA","#DIV/0!"))
pml_testing <- read.csv("pml_testing.csv", na.strings = c("","NA","#DIV/0!"))

# pml_building will be used to build the prediction model, whereas pml_testing will be used to predict the 'classe' variable. Both datasets will be cleaned first to remove any missing or irrelevant data.
```

### 2.2 Loading packages to be used

```{r, message=FALSE, warning=FALSE}
library(knitr)
library(caret)
library(xgboost)
library(Matrix)
library(DiagrammeR)
```

```{r setoptions}
opts_chunk$set(echo = TRUE, fig.width=7, fig.height=5, dev='svg', fig.path='Figs/', cache=TRUE)

options(scipen=999)
```

### 2.3 Cleaning the data

There are 160 variables in total in the `pml_building` data set, with some variables containing many NAs. To ensure the prediction model we are going to build is good, we remove columns containing more than 95% NAs and also those related with **row number**, **user name**, **times**, and **window** that we think are not good predictor variables. 

```{r, echo=TRUE}
## 160 variables in the data sets and some with many NAs
str(pml_building)
dim(pml_building)

## Keep columns containing less than 5% NAs
building <- pml_building[lapply(pml_building, function(x) sum(is.na(x)) / length(x) ) < 0.05]

testing <- pml_testing[lapply(pml_testing, function(x) sum(is.na(x)) / length(x)) < 0.05]

## Remove unlikely predictor variables, i.e., row number, user name, times, and window 
building_data <- building[,-c(1:7)]
testing_data <- testing[,-c(1:7)]
```

```{r}
dim(building_data); dim(testing_data)
```

After cleaning, we are left with 53 variables (including `classe`) in `building_data` and `testing_data`. In the following section, we employ the `XGBoost` algorithm to train a prediction model using `building_data` and test the model's performance. 

## 3. Modelling using `XGBoost` algorithm

`XGBoost` is used in this project because it is more powerful than a traditional Random Forest^[2]^. It implements the gradient boosting decision tree algorithm and it's the most popular implementation deployed in Kaggle Script to solve data science challenges. 

### 3.1 Random Subsampling

The `building_data` dataset is ramdonly split into 2 subsets using the `createDataPartition()` function in `caret` package. 70% of the original data is used for training, and 30% is used for testing. 

```{r}
## Unclass the factor 'classe' as XGBoost works only with numeric data 
building_data$classe <- unclass(building_data$classe)

## Subsampling
set.seed(23166)
inTrain <- createDataPartition(y = building_data$classe,
                               p = .7, list = FALSE)
subtraining <- building_data[inTrain,]
subtesting <- building_data[-inTrain,]

## Construct XGBoost input type; XGBoost is optimized for sparse input; label is the outcome of our dataset
trainingM <- sparse.model.matrix(classe ~ .-1, data = subtraining)
trainingM <- xgb.DMatrix(trainingM, label = subtraining$classe)

testingM <- sparse.model.matrix(classe ~ .-1, data = subtesting)
testingM <- xgb.DMatrix(testingM, label = subtesting$classe)
```

### 3.2 Fine-tuning parameters 

We run 5-fold cross validation 10 times, each time with random parameters from random set.seeds. The best parameter set is determined by whichever has the smallest CV-based evaluation error mean for the test CV-set.The `xgb.cv` function in `xgboost` package is used to cross validate the model performance. If the performance keeps getting worse consecutively for 8 rounds, training with a validation set will stop. 

```{r}
best_param = list()
best_seednumber = 1234
best_test_merror_mean = Inf
best_test_merror_mean_index = 0

for (iter in 1:10) {
    param <- list(objective = "multi:softmax",
          eval_metric = "merror",
          num_class = 6,
          max_depth = sample(6:10, 1),
          eta = runif(1, .01, .3),
          gamma = runif(1, 0.0, 0.2), 
          subsample = runif(1, .6, .9),
          colsample_bytree = runif(1, .5, .8), 
          min_child_weight = sample(1:40, 1),
          max_delta_step = sample(1:10, 1)
          )
   
seed.number = sample.int(10000, 1)[[1]]
set.seed(seed.number)
bst_cv <- xgb.cv(data = trainingM, 
                   params = param, 
                   nthread = 3, 
                   nfold = 5, 
                   nrounds = 200,
                   verbose = FALSE, 
                   early_stopping_rounds = 8, 
                   maximize = FALSE)

min_test_merror_mean = min(bst_cv$evaluation_log[, test_merror_mean])
min_test_merror_mean_index = which.min(bst_cv$evaluation_log[, test_merror_mean])

if (min_test_merror_mean < best_test_merror_mean) {
        best_test_merror_mean = min_test_merror_mean
        best_test_merror_mean_index = min_test_merror_mean_index
        best_seednumber = seed.number
        best_param = param
    }
}

best_parameter <- c(best_test_merror_mean_index,best_test_merror_mean,best_seednumber)
names(best_parameter) = c("nround","CV test error","Seed number")
kable(head(best_parameter), format = "html", align = "c", caption = "Best Parameter Set")
```


The lowest CV-based evaluation error is `r best_test_merror_mean` which happens at seed number `r best_seednumber` and nround of `r best_test_merror_mean_index`.

### 3.3 Training the model

We use nround = `r best_test_merror_mean_index` and Seed number = `r best_seednumber` to train the model. To do this, we employ the `xgb.train()` function in `xgboost` package.

```{r}
nround = best_test_merror_mean_index
set.seed(best_seednumber)
bst_Model <- xgb.train(data = trainingM, 
                params=best_param, 
                nrounds = nround, 
                nthread = 3)
```

### 3.4 Evaluating performance

We calculate the accuracy of the model on the validation data set `testingM`.

```{r}
testing_pred <- predict(bst_Model,testingM)
accuracy.validation <- confusionMatrix(testing_pred, subtesting$classe)$overall[1]
accuracy.validation
```

```{r, include=FALSE}
Out.of.sample.error <- 1 - accuracy.validation
```

As you can see, the accuracy is high (>99%), which indicates that our final model is pretty robust. Therefore, we obtain a very low out-of-sample error of `r Out.of.sample.error`. 
 
To further examine the model, we plot the top 10 important features in the model according to the gain value. Gain gives you indication about the information of how a feature is important in making a branch of a decision tree more pure. In this model, **yaw_belt**, **roll_belt**, and **pitch_forearm** are top 3 important features to predict the `classe` variable. 

```{r}
importance <- xgb.importance(dimnames(trainingM)[[2]],
                             model = bst_Model)

xgb.plot.importance(importance[1:10], 
                    xlab = "Gain")
```

The 2 first trees of the model are plotted below. 

```{r}
xgb.plot.tree(feature_names = dimnames(trainingM)[[2]], 
              model = bst_Model,
              trees = 2,
              plot_height = 1500, 
              plot_width = 1200)
```

## 4. Predicting the `testing_data` data set 

In the following code, we use the model we've obtained from above to predict the testing data from `pml_testing.csv`. 

```{r}
testM <- sparse.model.matrix(problem_id ~ .-1, 
                             data = testing_data)
test_pred <- predict(bst_Model, testM)
prediction_results <- chartr("12345", "ABCDE", test_pred)
prediction_results
```

## 5. Conclusion

In this project, we used the `xgboost` algorithm to build a prediction model based on 19622 observations from Weight Lifting Exercise Dataset. 

```{r, include=FALSE}
library('scales')
```

70% of the total observations were used to train the model while the remaining 30% were used for cross-validation. We ran 5-fold cross validation for 10 times. The model we finalised has an overall accuracy of `r percent(accuracy.validation)` on the validation test set; out of sample error is `r Out.of.sample.error`. 

We believe that the model is well developed for prediction.   

## References

[1] Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13). Stuttgart, Germany: ACM SIGCHI, 2013.

[2] Srivastava, T. [How to use XGBoost algorithm in R in easy steps](https://www.analyticsvidhya.com/blog/2016/01/xgboost-algorithm-easy-steps/). January 22, 2016.

[3] The Comprehensive R Archive Network - R Project. [XGBoost R Tutorial](https://cran.r-project.org/web/packages/xgboost/vignettes/xgboostPresentation.html).

<hr>